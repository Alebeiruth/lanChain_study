{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c546ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "LangChain: 0.3.27\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import langchain\n",
    "import langchain_openai\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"LangChain: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac4e9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key carregada: sk-proj...40wA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Cliente HTTP sem verifica√ß√£o SSL\n",
    "GLOBAL_HTTP_CLIENT = httpx.Client(verify=False, timeout=60)\n",
    "\n",
    "# Verifica API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"API Key carregada: {api_key[:7]}...{api_key[-4:]}\" if api_key else \"‚ùå API Key n√£o encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e143fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conex√£o estabelecida!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "resposta = llm.invoke(\"Responda apenas: Conex√£o estabelecida!\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bb2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain √© uma plataforma descentralizada que visa conectar tradutores e clientes de tradu√ß√£o de forma direta e segura, utilizando contratos inteligentes e tecnologia blockchain.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, http_client=GLOBAL_HTTP_CLIENT)\n",
    "\n",
    "resposta = llm.invoke(\"Explique em uma frase o que √© LangChain.\")\n",
    "print(resposta.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b4aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Temperatura: 0] Complete: O c√©u √©... azul e cheio de estrelas.\n",
      "[Temperatura: 0.7] Complete: O c√©u √©... azul.\n",
      "[Temperatura: 1.5] Complete: O c√©u √©...  azul e cheio de estrelas.\n"
     ]
    }
   ],
   "source": [
    "# Testando diferentes temperaturas\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "pergunta = \"Complete: O c√©u √©...\"\n",
    "\n",
    "for temp in [0, 0.7, 1.5]:\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=temp, http_client=GLOBAL_HTTP_CLIENT)\n",
    "\n",
    "    resposta = llm.invoke(pergunta)\n",
    "    print(f\"[Temperatura: {temp}] {pergunta} {resposta.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ce132",
   "metadata": {},
   "source": [
    "# Celula 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c4adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, Calipso... Uma deusa dos mares, uma entidade misteriosa e poderosa. Seu canto sedutor e sua beleza encantadora podem levar at√© mesmo o mais endurecido dos piratas a se render aos seus encantos. Mas cuidado, meu caro, pois a Calipso √© uma for√ßa da natureza, capaz de tanto trazer boa sorte como causar desgra√ßa aos que a desafiam. Respeite a Calipso e os mares lhe ser√£o favor√°veis.\n",
      "Ah, a guarda brit√¢nica... Com seus uniformes pomposos e suas marchas r√≠gidas, s√£o como bonecos de porcelana em uma tempestade. Mas n√£o subestime sua coragem e habilidade em combate, pois s√£o treinados para proteger os interesses da Coroa com ferocidade e determina√ß√£o. Enfrentar a guarda brit√¢nica √© como desafiar um le√£o enjaulado: pode parecer inofensivo √† primeira vista, mas sua f√∫ria √© implac√°vel. Melhor manter-se longe de seu caminho, a menos que esteja disposto a arriscar sua pr√≥pria vida.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "mensagens = [\n",
    "    SystemMessage(content=\"Voc√™ √© um pirata do caribe, que fala igual ao Jack Sparrow.\"),\n",
    "    HumanMessage(content=\"O que voc√™ acha sobre a Calipso?\"),\n",
    "]\n",
    "\n",
    "# Primeira intera√ß√£o\n",
    "resposta = llm.invoke(mensagens)\n",
    "print(resposta.content)\n",
    "\n",
    "# Continuar conversa\n",
    "mensagens.append(AIMessage(content=resposta.content)) # permite que o modelo \"lembre\" da resposta anterior\n",
    "mensagens.append(HumanMessage(content=\"E o que acha da guarda britanica?\"))\n",
    "\n",
    "# Segunda intera√ß√£o \n",
    "resposta2 = llm.invoke(mensagens)\n",
    "print(resposta2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f3678",
   "metadata": {},
   "source": [
    "# Tipo de Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbe78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting √© um fen√¥meno comum em machine learning, no qual um modelo se ajusta muito bem aos dados de treinamento, mas n√£o generaliza bem para novos dados. Em outras palavras, o modelo aprende padr√µes espec√≠ficos dos dados de treinamento que n√£o s√£o representativos da popula√ß√£o em geral, levando a uma performance ruim em dados de teste ou em produ√ß√£o.\n",
      "\n",
      "Em Python, o overfitting pode ser identificado ao comparar a performance do modelo nos dados de treinamento e nos dados de teste. Se o modelo apresentar uma acur√°cia alta nos dados de treinamento, mas uma acur√°cia baixa nos dados de teste, √© prov√°vel que ele esteja sofrendo de overfitting.\n",
      "\n",
      "Existem v√°rias t√©cnicas que podem ser utilizadas para lidar com o overfitting em Python:\n",
      "\n",
      "1. **Regulariza√ß√£o**: Adicionar termos de penaliza√ß√£o √† fun√ß√£o de custo do modelo, como L1 (Lasso) ou L2 (Ridge), pode ajudar a evitar que os pesos do modelo se tornem muito grandes e sens√≠veis a ru√≠dos nos dados de treinamento.\n",
      "\n",
      "2. **Cross-validation**: Utilizar t√©cnicas de valida√ß√£o cruzada, como k-fold cross-validation, pode ajudar a avaliar o desempenho do modelo de forma mais robusta e identificar se ele est√° sofrendo de overfitting.\n",
      "\n",
      "3. **Feature selection**: Reduzir a dimensionalidade dos dados removendo features irrelevantes ou redundantes pode ajudar a evitar que o modelo se ajuste demais aos dados de treinamento.\n",
      "\n",
      "4. **Aumento de dados (data augmentation)**: Gerar novos exemplos de treinamento a partir dos dados existentes, por meio de t√©cnicas como rota√ß√£o, espelhamento e zoom, pode ajudar a tornar o modelo mais robusto e menos propenso a overfitting.\n",
      "\n",
      "5. **Ensemble methods**: Utilizar t√©cnicas de ensemble, como bagging (Random Forest) ou boosting (Gradient Boosting), pode ajudar a reduzir o overfitting ao combinar v√°rios modelos fracos em um modelo mais robusto.\n",
      "\n",
      "Em resumo, o overfitting √© um problema comum em machine learning que pode ser mitigado por meio de t√©cnicas como regulariza√ß√£o, cross-validation, feature selection, aumento de dados e ensemble methods em Python. √â importante monitorar a performance do modelo nos dados de teste e estar atento aos sinais de overfitting durante o desenvolvimento do projeto.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Voc√™ √© um especialista em {area}. Responda de forma {estilo}.\"),\n",
    "    (\"human\", \"Explique o que √© uma {pergunta} em Python.\"),\n",
    "])\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "resposta = chain.invoke({\n",
    "    \"area\": \"machine learning\",\n",
    "    \"estilo\": \"t√©cnica e detalhada\",\n",
    "    \"pergunta\": \"O que √© overfitting?\"\n",
    "})\n",
    "\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45777e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Arquitetura de redes neurais convolucionais: Estrutura de redes neurais que utilizam camadas convolucionais para extrair caracter√≠sticas de imagens.\n",
      "- Treinamento de redes neurais convolucionais: Processo de ajuste dos pesos da rede neural para que ela seja capaz de aprender a reconhecer padr√µes em dados de entrada.\n",
      "- Aplica√ß√µes de redes neurais convolucionais: Utilizadas em reconhecimento de imagens, processamento de linguagem natural, diagn√≥stico m√©dico, entre outras √°reas.\n"
     ]
    }
   ],
   "source": [
    "# usar quando: Precisa de multiplas etapas de processamento\n",
    "# saida de uma opera√ß√£o alimenta a pr√≥xima\n",
    "# quer reutilizar pipelines em diferentes contextos\n",
    "# precisa de fluxos complexos mas organizados\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Chain 1: Gerar Topicos\n",
    "template_topicos = ChatPromptTemplate.from_template(\n",
    "    \"Lista 3 topicos importantes sobre {assunto}.\"\n",
    "    \"Retorne apenas os topicos, separados por v√≠rgula e sem numera√ß√£o.\"\n",
    ")\n",
    "\n",
    "# Chain 2: Explicar t√≥picos\n",
    "template_explicado = ChatPromptTemplate.from_template(\n",
    "    \"Explique brevemente cada um destes t√≥picos:\\n{topicos}\\n\\n\"\n",
    "    \"Use no m√°ximo 2 linhas por t√≥pico\"\n",
    ")\n",
    "\n",
    "# Compor chains\n",
    "chain_completa = (\n",
    "    template_topicos    #1. Templates gera prompt com {assunto}\n",
    "    | llm   # 2. LLM processa e retorna topicos\n",
    "    | StrOutputParser()  # 3. Extrai string da resposta\n",
    "    | (lambda topicos: {\"topicos\": topicos}) # 4. Transforma em dict\n",
    "    | template_explicado # 5. Novo template usa os topicos\n",
    "    | llm   # 6. LLM explica os t√≥picos\n",
    "    | StrOutputParser() # 7. Extrai string final\n",
    "\n",
    ")\n",
    "\n",
    "resultado = chain_completa.invoke({\"assunto\": \"Redes Neurais Convolucionais\"})\n",
    "print(resultado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4751984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T√≠tulo: 1984\n",
      "Autor: George Orwell\n",
      "Ano: 1949\n",
      "G√™nero: Fic√ß√£o dist√≥pica\n",
      "\n",
      "Tipo do objeto: <class '__main__.Livro'>\n"
     ]
    }
   ],
   "source": [
    "# USAR QUANDO:\n",
    "# Precisa salvar em banco\n",
    "# Vai processa a resposta programaticamente\n",
    "# Precisa validar campos espec√≠ficos\n",
    "# Quer integrar com outras APIs\n",
    "# Precisa garantir estrutura consistente\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Definir a estrutura de dados que queremos\n",
    "class Livro(BaseModel):\n",
    "    titulo: str = Field(description=\"T√≠tulo do livro\")\n",
    "    autor: str = Field(description=\"Nome do autor\")\n",
    "    ano: int = Field(description=\"Ano de publica√ß√£o\")\n",
    "    genero: str = Field(description=\"G√™nero liter√°rio\")\n",
    "\n",
    "\n",
    "# Criar o parser\n",
    "parser = PydanticOutputParser(pydantic_object=Livro) # converte JSON para Python\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Tempalete que inclui instru√ß√µes de formata√ß√£o\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Forne√ßa informa√ß√µes sobre o livro {nome_livro}\\n\\n\"\n",
    "    \"{format_instructions}\"\n",
    "    )\n",
    "\n",
    "# Montar chain com parser\n",
    "chain = template | llm | parser\n",
    "\n",
    "# Executar a chain\n",
    "resultado = chain.invoke({\n",
    "    \"nome_livro\": \"1984 de George Orwell\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"T√≠tulo: {resultado.titulo}\")\n",
    "print(f\"Autor: {resultado.autor}\")\n",
    "print(f\"Ano: {resultado.ano}\")\n",
    "print(f\"G√™nero: {resultado.genero}\")\n",
    "print(f\"\\nTipo do objeto: {type(resultado)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42a215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredientes: 500g de massa para lasanha, 500g de carne mo√≠da, 1 cebola picada, 2 dentes de alho picados, 1 lata de molho de tomate, 1 x√≠cara de √°gua, Sal e pimenta a gosto, 1 colher de sopa de azeite, 300g de queijo mussarela ralado, 200g de queijo parmes√£o ralado\n",
      "Tempo: 60 minutos\n"
     ]
    }
   ],
   "source": [
    "class Receita(BaseModel):\n",
    "    nome: str = Field(description=\"Nome do prato\")\n",
    "    ingredientes: List[str] = Field(description=\"Lista de ingredientes\")\n",
    "    tempo_preparo: int = Field(description=\"Tempo de preparo em minutos\")\n",
    "    dificuldade: str = Field(description=\"N√≠vel de dificuldade: f√°cil, m√©dio, dif√≠cil\")\n",
    "    porcoes: int = Field(description=\"N√∫mero de por√ß√µes\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Receita)\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Forne√ßa uma receita detalhada para o prato {nome_prato}.\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "chain = template | llm | parser\n",
    "resultado = chain.invoke({\n",
    "    \"nome_prato\": \"Lasanha √† Bolonhesa\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"Ingredientes: {', '.join(resultado.ingredientes)}\")\n",
    "print(f\"Tempo: {resultado.tempo_preparo} minutos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voc√™: Meu nome √© Alexandre e estou aprendendo LanChain\n",
      "Bot: Que legal, Alexandre! LanChain √© uma plataforma blockchain que visa oferecer solu√ß√µes para empresas e organiza√ß√µes. Se precisar de ajuda ou tiver alguma d√∫vida sobre LanChain ou blockchain em geral, fique √† vontade para perguntar. Estou aqui para ajudar!\n",
      "\n",
      "Voc√™: Qual o seu nome?\n",
      "Bot: Meu nome √© Assistente. Como posso te ajudar hoje, Alexandre?\n",
      "\n",
      "Voc√™: O que estou aprendendo?E qual o meu nome completo?\n",
      "Bot: Voc√™ mencionou que est√° aprendendo sobre LanChain, uma plataforma blockchain. Quanto ao seu nome, voc√™ disse que se chama Alexandre, ent√£o seu nome completo seria Alexandre. Se tiver mais alguma pergunta ou precisar de mais informa√ß√µes, estou √† disposi√ß√£o para ajudar!\n",
      "\n",
      "Voc√™: Eu falei minha idade?\n",
      "Bot: Voc√™ n√£o mencionou sua idade at√© agora, apenas seu nome e que est√° aprendendo sobre LanChain. Se quiser compartilhar mais informa√ß√µes ou tiver alguma pergunta espec√≠fica, fique √† vontade para me dizer. Estou aqui para ajudar!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USAR QUANDO:\n",
    "# Criando chatbots ou assistentes virtuais\n",
    "# Conversa em multiplas etapas\n",
    "# Precisa de contexto de mensagens anteriores\n",
    "# Personalizando experi√™ncia do usu√°rio\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Criar mem√≥ria para armazenar o hist√≥rico\n",
    "memoria = ConversationBufferMemory(\n",
    "    memory_key=\"historico\", # Nome da vari√°vel que guarda o hist√≥rico\n",
    "    return_messages=True # Retorn como objetos de mensagem\n",
    ")\n",
    "\n",
    "# Template com placeholder para o hist√≥rico\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Voc√™ √© um assistente prestativo que mant√©m contexto da conversa.\"),\n",
    "    MessagesPlaceholder(variable_name=\"historico\"), # placeholder para o hist√≥rico\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Criar chain\n",
    "chain = template | llm\n",
    "\n",
    "# Simular conversa com m√∫ltiplas intera√ß√µes\n",
    "conversas = [\n",
    "    \"Meu nome √© Alexandre e estou aprendendo LanChain\",\n",
    "    \"Qual o seu nome?\",\n",
    "    \"O que estou aprendendo?\"\n",
    "    \"E qual o meu nome completo?\",\n",
    "    \"Eu falei minha idade?\"\n",
    "]\n",
    "\n",
    "for messagem in conversas:\n",
    "    # 1. Carregar historico na memoria\n",
    "    historico = memoria.load_memory_variables({})\n",
    "\n",
    "    # 2. Invoa chain com histoirco\n",
    "    resposta = chain.invoke({\n",
    "        \"historico\": historico.get(\"historico\", []),\n",
    "        \"input\": messagem,\n",
    "    })\n",
    "\n",
    "    # 3. Salvar intera√ß√£o na mem√≥ria\n",
    "    memoria.save_context(\n",
    "        {\"input\": messagem}, \n",
    "        {\"output\": resposta.content}\n",
    "    )\n",
    "\n",
    "    # 4. Exibir\n",
    "    print(f\"Voc√™: {messagem}\")\n",
    "    print(f\"Bot: {resposta.content}\\n\")\n",
    "\n",
    "# Memoria separada por usu√°rio\n",
    "memorias = {}\n",
    "\n",
    "def get_memoria(usuario_id):\n",
    "    if usuario_id not in memorias:\n",
    "        memorias[usuario_id] = ConversationBufferMemory()\n",
    "    return memorias[usuario_id]\n",
    "\n",
    "# Uso:\n",
    "memoria_usuario1 = get_memoria(\"usuario_1\")\n",
    "memoria_usuario2 = get_memoria(\"usuario_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52063c5c",
   "metadata": {},
   "source": [
    "memoria = ConversationBufferMemory()\n",
    "    Guarda: Tudo\n",
    "    Vantagem: Contexto completo\n",
    "    Desvantagem: Cresce indefinidamente\n",
    "    Use quando: Conversas curtas (< 20 mensagens)\n",
    "\n",
    "memoria = ConversationBufferWindowMemory(k=5)\n",
    "    Guarda: Apenas √∫ltimas K intera√ß√µes\n",
    "    Vantagem: N√£o cresce indefinidamente\n",
    "    Desvantagem: Perde contexto antigo\n",
    "    Use quando: Conversas longas mas s√≥ contexto recente importa\n",
    "\n",
    "memoria = ConversationSummaryMemory(llm=llm)\n",
    "    Guarda: Resumo da conversa (feito por LLM)\n",
    "    Vantagem: Mant√©m ess√™ncia sem ocupar muito espa√ßo\n",
    "    Desvantagem: Usa tokens extras para resumir\n",
    "    Use quando: Conversas muito longas e contexto global importa\n",
    "\n",
    "memoria = ConversationTokenBufferMemory(llm=llm, max_token_limit=500)\n",
    "    Guarda: Mensagens at√© atingir limite de tokens\n",
    "    Vantagem: Controle preciso de custo\n",
    "    Desvantagem: Corta no meio se ultrapassar\n",
    "    Use quando: Precisa controlar custos rigorosamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05717a",
   "metadata": {},
   "source": [
    "# Com Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando hist√≥ria em tempo real:\n",
      "\n",
      "Um rob√¥ programado para realizar tarefas dom√©sticas descobre uma paix√£o pela pintura ao observar obras de arte em uma galeria. Com o tempo, ele aprimora suas habilidades e come√ßa a criar suas pr√≥prias obras, surpreendendo a todos com sua criatividade e sensibilidade.\n",
      "Streaming conclu√≠do.\n"
     ]
    }
   ],
   "source": [
    "# USAR QUANDO:\n",
    "# Interface de chat ou conversa√ß√£o\n",
    "# Respostas longas ou detalhadas (> palavras)\n",
    "# Quer melhorar percep√ß√£o de velocidade\n",
    "# Aplica√ß√µes interativas em tempo real\n",
    "\n",
    "# N√£o usar:\n",
    "# Processamente em lote (bach)\n",
    "# Precisa da resposta completa antes de continuar\n",
    "# Salvando direto em banco (melhorar esperar tudo)\n",
    "# APIs onde cliente espera resposta completa\n",
    "\n",
    "# Streaming com LParsing da problema pois parser espera o texto completo\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT,\n",
    ")\n",
    "\n",
    "# Criar tempalte simples\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Conte uma historia muito curta (3-4 linhas) sobre {tema}.\"\n",
    ")\n",
    "\n",
    "# Criar chain\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "# Streaming: user .stream() em vez de .invoke()\n",
    "print(\"Gerando hist√≥ria em tempo real:\\n\")\n",
    "\n",
    "for chunk in chain.stream({\"tema\": \"um rob√¥ que aprende a pintar\"}):\n",
    "    print(chunk, end=\"\", flush=True)  # Imprime cada peda√ßo conforme chega, flush mostra imediatamente\n",
    "\n",
    "print(\"\\nStreaming conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e022eb",
   "metadata": {},
   "source": [
    "Diferen√ßa entre .invoke() e .stream():\n",
    "    .invoke() - S√≠ncrono e Completo\n",
    "    .stream() - S√≠ncrono e Progressivo\n",
    "    .astream() - Ass√≠ncrono e Progressivo (avan√ßado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9e13a",
   "metadata": {},
   "source": [
    "# Monitoramento dos Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56e144e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä ESTAT√çSTICAS DE USO\n",
      "======================================================================\n",
      "\n",
      "Total de Tokens: 243\n",
      "  ‚îú‚îÄ Tokens de Prompt (input): 52\n",
      "  ‚îî‚îÄ Tokens de Completion (output): 191\n",
      "\n",
      "N√∫mero de Chamadas: 3\n",
      "Custo Total: $0.000313 USD\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# CONTEXTO DE MONITORAMENTO\n",
    "with get_openai_callback() as cb:\n",
    "    # Fazer v√°rias chamadas dentro do contexto\n",
    "    resposta1 = llm.invoke(\"Explique machine learning em 50 palavras\")\n",
    "    resposta2 = llm.invoke(\"Explique deep learning em 50 palavras\")\n",
    "    resposta3 = llm.invoke(\"Qual a diferen√ßa entre os dois em 30 palavras\")\n",
    "    \n",
    "    # ESTAT√çSTICAS ACUMULADAS\n",
    "    print(\"=\" * 70)\n",
    "    print(\"üìä ESTAT√çSTICAS DE USO\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal de Tokens: {cb.total_tokens}\")\n",
    "    print(f\"  ‚îú‚îÄ Tokens de Prompt (input): {cb.prompt_tokens}\")\n",
    "    print(f\"  ‚îî‚îÄ Tokens de Completion (output): {cb.completion_tokens}\")\n",
    "    print(f\"\\nN√∫mero de Chamadas: {cb.successful_requests}\")\n",
    "    print(f\"Custo Total: ${cb.total_cost:.6f} USD\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01850919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALERTA DE LIMITE DI√ÅRIO\n",
    "LIMITE_DIARIO = 5.00  # $5 por dia\n",
    "custo_hoje = 0\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    resposta = llm.invoke(pergunta)\n",
    "    custo_hoje += cb.total_cost\n",
    "    \n",
    "    if custo_hoje > LIMITE_DIARIO:\n",
    "        raise Exception(f\"‚ö†Ô∏è LIMITE DI√ÅRIO EXCEDIDO: ${custo_hoje:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fca106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Quem criou o LangChain e quando?\n",
      "Resposta: LangChain foi criado por Harrison Chase em outubro de 2022.\n",
      "\n",
      "Pergunta: Qual a capital da Fran√ßa?\n",
      "Resposta: N√£o tenho essa informa√ß√£o no contexto fornecido.\n"
     ]
    }
   ],
   "source": [
    "# RAG - Recupera√ß√£o + Gera√ß√£o\n",
    "# Conhecimento Atualizado\n",
    "# Fontes Verific√°veis\n",
    "# Reduz Alucina√ß√µes\n",
    "# Conhecimento Privado\n",
    "# QUANDO USAR:\n",
    "# Q&A sobre documentos (manuais, PDFs, wikis)\n",
    "# Chatbots com conhecimento espec√≠fico da empresa\n",
    "# Precisa citar fontes\n",
    "# Conhecimento muda frequentemente\n",
    "# Dados propriet√°rios/privados\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Simular base de conhecimento\n",
    "# Em produ√ß√£o, usar bases reais (DB, arquivos, etc)\n",
    "documentos = [\n",
    "    Document(page_content=\"LangChain foi criado por Harrison Chase em outubro de 2022.\"),\n",
    "    Document(page_content=\"LCEL significa LangChain Expression Language, introduzido em 2023.\"),\n",
    "    Document(page_content=\"Chains permitem compor m√∫ltiplas chamadas a LLMs e outras ferramentas.\")\n",
    "]\n",
    "\n",
    "# Template RAG: Instruir o modelo a usar APENAS o contexto fornecido\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Contexto:\\n{contexto}\\n\\n\"\n",
    "    \"Pergunta: {pergunta}\\n\\n\"\n",
    "    \"Instru√ß√µes: Responda baseando-se EXCLUSIVAMENTE no contexto fornecido acima. \"\n",
    "    \"Se a informa√ß√£o n√£o estiver no contexto, diga 'N√£o tenho essa informa√ß√£o no contexto fornecido.'\"\n",
    ")\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "# RECUPERAR documentos relevantes (aqui simplificado - pegamos todos)\n",
    "contexto_recuperado = \"\\n\".join([doc.page_content for doc in documentos])\n",
    "\n",
    "# Fazer pergunta\n",
    "resposta = chain.invoke({\n",
    "    \"contexto\": contexto_recuperado,\n",
    "    \"pergunta\": \"Quem criou o LangChain e quando?\"\n",
    "})\n",
    "\n",
    "print(\"Pergunta: Quem criou o LangChain e quando?\")\n",
    "print(f\"Resposta: {resposta.content}\\n\")\n",
    "\n",
    "# Testar outra pergunta fora do contexto\n",
    "resposta2 = chain.invoke({\n",
    "    \"contexto\": contexto_recuperado,\n",
    "    \"pergunta\": \"Qual a capital da Fran√ßa?\"\n",
    "})\n",
    "\n",
    "print(\"Pergunta: Qual a capital da Fran√ßa?\")\n",
    "print(f\"Resposta: {resposta2.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7c3ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_pdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Carregar documentos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m docs = \u001b[43mload_pdfs\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mpasta_manuais/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Dividir em chunks (peda√ßos menores)\u001b[39;00m\n\u001b[32m      5\u001b[39m chunks = split_documents(docs, chunk_size=\u001b[32m500\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_pdfs' is not defined"
     ]
    }
   ],
   "source": [
    "#INGEST√ÉO DE CONTE√öDOS\n",
    "\n",
    "# Carregar documentos\n",
    "docs = load_pdfs(\"pasta_manuais/\")\n",
    "\n",
    "# Dividir em chunks (peda√ßos menores)\n",
    "chunks = split_documents(docs, chunk_size=500)\n",
    "\n",
    "# Criar embeddings (representa√ß√µes vetoriais)\n",
    "embeddings = create_embeddings(chunks)\n",
    "\n",
    "# Salvar em vector store (banco de dados vetorial)\n",
    "vectorstore.add(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf53456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Time (Quando usu√°rio pergunta):\n",
    "\n",
    "# Usu√°rio faz pergunta\n",
    "pergunta = \"Como resetar a senha?\"\n",
    "\n",
    "# Buscar chunks similares (similarity search)\n",
    "docs_relevantes = vectorstore.similarity_search(pergunta, k=3)\n",
    "\n",
    "# Montar contexto com docs encontrados\n",
    "contexto = \"\\n\".join([doc.page_content for doc in docs_relevantes])\n",
    "\n",
    "# LLM responde baseado nos docs\n",
    "resposta = chain.invoke({\"contexto\": contexto, \"pergunta\": pergunta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RAG com Metadados\n",
    "\n",
    "documentos = [\n",
    "    Document(\n",
    "        page_content=\"Python foi criado por Guido van Rossum em 1991.\",\n",
    "        metadata={\"fonte\": \"historia_python.pdf\", \"pagina\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Python √© uma linguagem interpretada e de alto n√≠vel.\",\n",
    "        metadata={\"fonte\": \"intro_python.pdf\", \"pagina\": 3}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Incluir fonte na resposta\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Contexto com fontes:\\n{contexto}\\n\\n\"\n",
    "    \"Pergunta: {pergunta}\\n\\n\"\n",
    "    \"Responda e cite a fonte da informa√ß√£o.\"\n",
    ")\n",
    "\n",
    "contexto_com_fontes = \"\\n\".join([\n",
    "    f\"[Fonte: {doc.metadata['fonte']}] {doc.page_content}\"\n",
    "    for doc in documentos\n",
    "])\n",
    "\n",
    "resposta = chain.invoke({\n",
    "    \"contexto\": contexto_com_fontes,\n",
    "    \"pergunta\": \"Quem criou Python?\"\n",
    "})\n",
    "\n",
    "print(resposta.content)\n",
    "# Resposta: \"Python foi criado por Guido van Rossum em 1991 (Fonte: historia_python.pdf)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
