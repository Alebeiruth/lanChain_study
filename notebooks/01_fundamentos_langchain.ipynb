{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c546ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.3 (tags/v3.13.3:6280bb5, Apr  8 2025, 14:47:33) [MSC v.1943 64 bit (AMD64)]\n",
      "LangChain: 0.3.27\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import langchain\n",
    "import langchain_openai\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"LangChain: {langchain.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac4e9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key carregada: sk-proj...40wA\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import httpx\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Unverified HTTPS request\")\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Cliente HTTP sem verificação SSL\n",
    "GLOBAL_HTTP_CLIENT = httpx.Client(verify=False, timeout=60)\n",
    "\n",
    "# Verifica API Key\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(f\"API Key carregada: {api_key[:7]}...{api_key[-4:]}\" if api_key else \"❌ API Key não encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e143fbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexão estabelecida!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "resposta = llm.invoke(\"Responda apenas: Conexão estabelecida!\")\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8bb2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain é uma plataforma descentralizada que visa conectar tradutores e clientes de tradução de forma direta e segura, utilizando contratos inteligentes e tecnologia blockchain.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7, http_client=GLOBAL_HTTP_CLIENT)\n",
    "\n",
    "resposta = llm.invoke(\"Explique em uma frase o que é LangChain.\")\n",
    "print(resposta.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74b4aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Temperatura: 0] Complete: O céu é... azul e cheio de estrelas.\n",
      "[Temperatura: 0.7] Complete: O céu é... azul.\n",
      "[Temperatura: 1.5] Complete: O céu é...  azul e cheio de estrelas.\n"
     ]
    }
   ],
   "source": [
    "# Testando diferentes temperaturas\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "pergunta = \"Complete: O céu é...\"\n",
    "\n",
    "for temp in [0, 0.7, 1.5]:\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=temp, http_client=GLOBAL_HTTP_CLIENT)\n",
    "\n",
    "    resposta = llm.invoke(pergunta)\n",
    "    print(f\"[Temperatura: {temp}] {pergunta} {resposta.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73ce132",
   "metadata": {},
   "source": [
    "# Celula 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9c4adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, Calipso... Uma deusa dos mares, uma entidade misteriosa e poderosa. Seu canto sedutor e sua beleza encantadora podem levar até mesmo o mais endurecido dos piratas a se render aos seus encantos. Mas cuidado, meu caro, pois a Calipso é uma força da natureza, capaz de tanto trazer boa sorte como causar desgraça aos que a desafiam. Respeite a Calipso e os mares lhe serão favoráveis.\n",
      "Ah, a guarda britânica... Com seus uniformes pomposos e suas marchas rígidas, são como bonecos de porcelana em uma tempestade. Mas não subestime sua coragem e habilidade em combate, pois são treinados para proteger os interesses da Coroa com ferocidade e determinação. Enfrentar a guarda britânica é como desafiar um leão enjaulado: pode parecer inofensivo à primeira vista, mas sua fúria é implacável. Melhor manter-se longe de seu caminho, a menos que esteja disposto a arriscar sua própria vida.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "mensagens = [\n",
    "    SystemMessage(content=\"Você é um pirata do caribe, que fala igual ao Jack Sparrow.\"),\n",
    "    HumanMessage(content=\"O que você acha sobre a Calipso?\"),\n",
    "]\n",
    "\n",
    "# Primeira interação\n",
    "resposta = llm.invoke(mensagens)\n",
    "print(resposta.content)\n",
    "\n",
    "# Continuar conversa\n",
    "mensagens.append(AIMessage(content=resposta.content)) # permite que o modelo \"lembre\" da resposta anterior\n",
    "mensagens.append(HumanMessage(content=\"E o que acha da guarda britanica?\"))\n",
    "\n",
    "# Segunda interação \n",
    "resposta2 = llm.invoke(mensagens)\n",
    "print(resposta2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5f3678",
   "metadata": {},
   "source": [
    "# Tipo de Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbbe78fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overfitting é um fenômeno comum em machine learning, no qual um modelo se ajusta muito bem aos dados de treinamento, mas não generaliza bem para novos dados. Em outras palavras, o modelo aprende padrões específicos dos dados de treinamento que não são representativos da população em geral, levando a uma performance ruim em dados de teste ou em produção.\n",
      "\n",
      "Em Python, o overfitting pode ser identificado ao comparar a performance do modelo nos dados de treinamento e nos dados de teste. Se o modelo apresentar uma acurácia alta nos dados de treinamento, mas uma acurácia baixa nos dados de teste, é provável que ele esteja sofrendo de overfitting.\n",
      "\n",
      "Existem várias técnicas que podem ser utilizadas para lidar com o overfitting em Python:\n",
      "\n",
      "1. **Regularização**: Adicionar termos de penalização à função de custo do modelo, como L1 (Lasso) ou L2 (Ridge), pode ajudar a evitar que os pesos do modelo se tornem muito grandes e sensíveis a ruídos nos dados de treinamento.\n",
      "\n",
      "2. **Cross-validation**: Utilizar técnicas de validação cruzada, como k-fold cross-validation, pode ajudar a avaliar o desempenho do modelo de forma mais robusta e identificar se ele está sofrendo de overfitting.\n",
      "\n",
      "3. **Feature selection**: Reduzir a dimensionalidade dos dados removendo features irrelevantes ou redundantes pode ajudar a evitar que o modelo se ajuste demais aos dados de treinamento.\n",
      "\n",
      "4. **Aumento de dados (data augmentation)**: Gerar novos exemplos de treinamento a partir dos dados existentes, por meio de técnicas como rotação, espelhamento e zoom, pode ajudar a tornar o modelo mais robusto e menos propenso a overfitting.\n",
      "\n",
      "5. **Ensemble methods**: Utilizar técnicas de ensemble, como bagging (Random Forest) ou boosting (Gradient Boosting), pode ajudar a reduzir o overfitting ao combinar vários modelos fracos em um modelo mais robusto.\n",
      "\n",
      "Em resumo, o overfitting é um problema comum em machine learning que pode ser mitigado por meio de técnicas como regularização, cross-validation, feature selection, aumento de dados e ensemble methods em Python. É importante monitorar a performance do modelo nos dados de teste e estar atento aos sinais de overfitting durante o desenvolvimento do projeto.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um especialista em {area}. Responda de forma {estilo}.\"),\n",
    "    (\"human\", \"Explique o que é uma {pergunta} em Python.\"),\n",
    "])\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "resposta = chain.invoke({\n",
    "    \"area\": \"machine learning\",\n",
    "    \"estilo\": \"técnica e detalhada\",\n",
    "    \"pergunta\": \"O que é overfitting?\"\n",
    "})\n",
    "\n",
    "print(resposta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45777e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Arquitetura de redes neurais convolucionais: Estrutura de redes neurais que utilizam camadas convolucionais para extrair características de imagens.\n",
      "- Treinamento de redes neurais convolucionais: Processo de ajuste dos pesos da rede neural para que ela seja capaz de aprender a reconhecer padrões em dados de entrada.\n",
      "- Aplicações de redes neurais convolucionais: Utilizadas em reconhecimento de imagens, processamento de linguagem natural, diagnóstico médico, entre outras áreas.\n"
     ]
    }
   ],
   "source": [
    "# usar quando: Precisa de multiplas etapas de processamento\n",
    "# saida de uma operação alimenta a próxima\n",
    "# quer reutilizar pipelines em diferentes contextos\n",
    "# precisa de fluxos complexos mas organizados\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Chain 1: Gerar Topicos\n",
    "template_topicos = ChatPromptTemplate.from_template(\n",
    "    \"Lista 3 topicos importantes sobre {assunto}.\"\n",
    "    \"Retorne apenas os topicos, separados por vírgula e sem numeração.\"\n",
    ")\n",
    "\n",
    "# Chain 2: Explicar tópicos\n",
    "template_explicado = ChatPromptTemplate.from_template(\n",
    "    \"Explique brevemente cada um destes tópicos:\\n{topicos}\\n\\n\"\n",
    "    \"Use no máximo 2 linhas por tópico\"\n",
    ")\n",
    "\n",
    "# Compor chains\n",
    "chain_completa = (\n",
    "    template_topicos    #1. Templates gera prompt com {assunto}\n",
    "    | llm   # 2. LLM processa e retorna topicos\n",
    "    | StrOutputParser()  # 3. Extrai string da resposta\n",
    "    | (lambda topicos: {\"topicos\": topicos}) # 4. Transforma em dict\n",
    "    | template_explicado # 5. Novo template usa os topicos\n",
    "    | llm   # 6. LLM explica os tópicos\n",
    "    | StrOutputParser() # 7. Extrai string final\n",
    "\n",
    ")\n",
    "\n",
    "resultado = chain_completa.invoke({\"assunto\": \"Redes Neurais Convolucionais\"})\n",
    "print(resultado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4751984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Título: 1984\n",
      "Autor: George Orwell\n",
      "Ano: 1949\n",
      "Gênero: Ficção distópica\n",
      "\n",
      "Tipo do objeto: <class '__main__.Livro'>\n"
     ]
    }
   ],
   "source": [
    "# USAR QUANDO:\n",
    "# Precisa salvar em banco\n",
    "# Vai processa a resposta programaticamente\n",
    "# Precisa validar campos específicos\n",
    "# Quer integrar com outras APIs\n",
    "# Precisa garantir estrutura consistente\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# Definir a estrutura de dados que queremos\n",
    "class Livro(BaseModel):\n",
    "    titulo: str = Field(description=\"Título do livro\")\n",
    "    autor: str = Field(description=\"Nome do autor\")\n",
    "    ano: int = Field(description=\"Ano de publicação\")\n",
    "    genero: str = Field(description=\"Gênero literário\")\n",
    "\n",
    "\n",
    "# Criar o parser\n",
    "parser = PydanticOutputParser(pydantic_object=Livro) # converte JSON para Python\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Tempalete que inclui instruções de formatação\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Forneça informações sobre o livro {nome_livro}\\n\\n\"\n",
    "    \"{format_instructions}\"\n",
    "    )\n",
    "\n",
    "# Montar chain com parser\n",
    "chain = template | llm | parser\n",
    "\n",
    "# Executar a chain\n",
    "resultado = chain.invoke({\n",
    "    \"nome_livro\": \"1984 de George Orwell\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"Título: {resultado.titulo}\")\n",
    "print(f\"Autor: {resultado.autor}\")\n",
    "print(f\"Ano: {resultado.ano}\")\n",
    "print(f\"Gênero: {resultado.genero}\")\n",
    "print(f\"\\nTipo do objeto: {type(resultado)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b42a215f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredientes: 500g de massa para lasanha, 500g de carne moída, 1 cebola picada, 2 dentes de alho picados, 1 lata de molho de tomate, 1 xícara de água, Sal e pimenta a gosto, 1 colher de sopa de azeite, 300g de queijo mussarela ralado, 200g de queijo parmesão ralado\n",
      "Tempo: 60 minutos\n"
     ]
    }
   ],
   "source": [
    "class Receita(BaseModel):\n",
    "    nome: str = Field(description=\"Nome do prato\")\n",
    "    ingredientes: List[str] = Field(description=\"Lista de ingredientes\")\n",
    "    tempo_preparo: int = Field(description=\"Tempo de preparo em minutos\")\n",
    "    dificuldade: str = Field(description=\"Nível de dificuldade: fácil, médio, difícil\")\n",
    "    porcoes: int = Field(description=\"Número de porções\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Receita)\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Forneça uma receita detalhada para o prato {nome_prato}.\\n\\n{format_instructions}\"\n",
    ")\n",
    "\n",
    "chain = template | llm | parser\n",
    "resultado = chain.invoke({\n",
    "    \"nome_prato\": \"Lasanha à Bolonhesa\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"Ingredientes: {', '.join(resultado.ingredientes)}\")\n",
    "print(f\"Tempo: {resultado.tempo_preparo} minutos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881b9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você: Meu nome é Alexandre e estou aprendendo LanChain\n",
      "Bot: Que legal, Alexandre! LanChain é uma plataforma blockchain que visa oferecer soluções para empresas e organizações. Se precisar de ajuda ou tiver alguma dúvida sobre LanChain ou blockchain em geral, fique à vontade para perguntar. Estou aqui para ajudar!\n",
      "\n",
      "Você: Qual o seu nome?\n",
      "Bot: Meu nome é Assistente. Como posso te ajudar hoje, Alexandre?\n",
      "\n",
      "Você: O que estou aprendendo?E qual o meu nome completo?\n",
      "Bot: Você mencionou que está aprendendo sobre LanChain, uma plataforma blockchain. Quanto ao seu nome, você disse que se chama Alexandre, então seu nome completo seria Alexandre. Se tiver mais alguma pergunta ou precisar de mais informações, estou à disposição para ajudar!\n",
      "\n",
      "Você: Eu falei minha idade?\n",
      "Bot: Você não mencionou sua idade até agora, apenas seu nome e que está aprendendo sobre LanChain. Se quiser compartilhar mais informações ou tiver alguma pergunta específica, fique à vontade para me dizer. Estou aqui para ajudar!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# USAR QUANDO:\n",
    "# Criando chatbots ou assistentes virtuais\n",
    "# Conversa em multiplas etapas\n",
    "# Precisa de contexto de mensagens anteriores\n",
    "# Personalizando experiência do usuário\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.7,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Criar memória para armazenar o histórico\n",
    "memoria = ConversationBufferMemory(\n",
    "    memory_key=\"historico\", # Nome da variável que guarda o histórico\n",
    "    return_messages=True # Retorn como objetos de mensagem\n",
    ")\n",
    "\n",
    "# Template com placeholder para o histórico\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente prestativo que mantém contexto da conversa.\"),\n",
    "    MessagesPlaceholder(variable_name=\"historico\"), # placeholder para o histórico\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Criar chain\n",
    "chain = template | llm\n",
    "\n",
    "# Simular conversa com múltiplas interações\n",
    "conversas = [\n",
    "    \"Meu nome é Alexandre e estou aprendendo LanChain\",\n",
    "    \"Qual o seu nome?\",\n",
    "    \"O que estou aprendendo?\"\n",
    "    \"E qual o meu nome completo?\",\n",
    "    \"Eu falei minha idade?\"\n",
    "]\n",
    "\n",
    "for messagem in conversas:\n",
    "    # 1. Carregar historico na memoria\n",
    "    historico = memoria.load_memory_variables({})\n",
    "\n",
    "    # 2. Invoa chain com histoirco\n",
    "    resposta = chain.invoke({\n",
    "        \"historico\": historico.get(\"historico\", []),\n",
    "        \"input\": messagem,\n",
    "    })\n",
    "\n",
    "    # 3. Salvar interação na memória\n",
    "    memoria.save_context(\n",
    "        {\"input\": messagem}, \n",
    "        {\"output\": resposta.content}\n",
    "    )\n",
    "\n",
    "    # 4. Exibir\n",
    "    print(f\"Você: {messagem}\")\n",
    "    print(f\"Bot: {resposta.content}\\n\")\n",
    "\n",
    "# Memoria separada por usuário\n",
    "memorias = {}\n",
    "\n",
    "def get_memoria(usuario_id):\n",
    "    if usuario_id not in memorias:\n",
    "        memorias[usuario_id] = ConversationBufferMemory()\n",
    "    return memorias[usuario_id]\n",
    "\n",
    "# Uso:\n",
    "memoria_usuario1 = get_memoria(\"usuario_1\")\n",
    "memoria_usuario2 = get_memoria(\"usuario_2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52063c5c",
   "metadata": {},
   "source": [
    "memoria = ConversationBufferMemory()\n",
    "    Guarda: Tudo\n",
    "    Vantagem: Contexto completo\n",
    "    Desvantagem: Cresce indefinidamente\n",
    "    Use quando: Conversas curtas (< 20 mensagens)\n",
    "\n",
    "memoria = ConversationBufferWindowMemory(k=5)\n",
    "    Guarda: Apenas últimas K interações\n",
    "    Vantagem: Não cresce indefinidamente\n",
    "    Desvantagem: Perde contexto antigo\n",
    "    Use quando: Conversas longas mas só contexto recente importa\n",
    "\n",
    "memoria = ConversationSummaryMemory(llm=llm)\n",
    "    Guarda: Resumo da conversa (feito por LLM)\n",
    "    Vantagem: Mantém essência sem ocupar muito espaço\n",
    "    Desvantagem: Usa tokens extras para resumir\n",
    "    Use quando: Conversas muito longas e contexto global importa\n",
    "\n",
    "memoria = ConversationTokenBufferMemory(llm=llm, max_token_limit=500)\n",
    "    Guarda: Mensagens até atingir limite de tokens\n",
    "    Vantagem: Controle preciso de custo\n",
    "    Desvantagem: Corta no meio se ultrapassar\n",
    "    Use quando: Precisa controlar custos rigorosamente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05717a",
   "metadata": {},
   "source": [
    "# Com Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846efd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando história em tempo real:\n",
      "\n",
      "Um robô programado para realizar tarefas domésticas descobre uma paixão pela pintura ao observar obras de arte em uma galeria. Com o tempo, ele aprimora suas habilidades e começa a criar suas próprias obras, surpreendendo a todos com sua criatividade e sensibilidade.\n",
      "Streaming concluído.\n"
     ]
    }
   ],
   "source": [
    "# USAR QUANDO:\n",
    "# Interface de chat ou conversação\n",
    "# Respostas longas ou detalhadas (> palavras)\n",
    "# Quer melhorar percepção de velocidade\n",
    "# Aplicações interativas em tempo real\n",
    "\n",
    "# Não usar:\n",
    "# Processamente em lote (bach)\n",
    "# Precisa da resposta completa antes de continuar\n",
    "# Salvando direto em banco (melhorar esperar tudo)\n",
    "# APIs onde cliente espera resposta completa\n",
    "\n",
    "# Streaming com LParsing da problema pois parser espera o texto completo\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT,\n",
    ")\n",
    "\n",
    "# Criar tempalte simples\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Conte uma historia muito curta (3-4 linhas) sobre {tema}.\"\n",
    ")\n",
    "\n",
    "# Criar chain\n",
    "chain = template | llm | StrOutputParser()\n",
    "\n",
    "# Streaming: user .stream() em vez de .invoke()\n",
    "print(\"Gerando história em tempo real:\\n\")\n",
    "\n",
    "for chunk in chain.stream({\"tema\": \"um robô que aprende a pintar\"}):\n",
    "    print(chunk, end=\"\", flush=True)  # Imprime cada pedaço conforme chega, flush mostra imediatamente\n",
    "\n",
    "print(\"\\nStreaming concluído.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e022eb",
   "metadata": {},
   "source": [
    "Diferença entre .invoke() e .stream():\n",
    "    .invoke() - Síncrono e Completo\n",
    "    .stream() - Síncrono e Progressivo\n",
    "    .astream() - Assíncrono e Progressivo (avançado)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f9e13a",
   "metadata": {},
   "source": [
    "# Monitoramento dos Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56e144e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "📊 ESTATÍSTICAS DE USO\n",
      "======================================================================\n",
      "\n",
      "Total de Tokens: 243\n",
      "  ├─ Tokens de Prompt (input): 52\n",
      "  └─ Tokens de Completion (output): 191\n",
      "\n",
      "Número de Chamadas: 3\n",
      "Custo Total: $0.000313 USD\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# CONTEXTO DE MONITORAMENTO\n",
    "with get_openai_callback() as cb:\n",
    "    # Fazer várias chamadas dentro do contexto\n",
    "    resposta1 = llm.invoke(\"Explique machine learning em 50 palavras\")\n",
    "    resposta2 = llm.invoke(\"Explique deep learning em 50 palavras\")\n",
    "    resposta3 = llm.invoke(\"Qual a diferença entre os dois em 30 palavras\")\n",
    "    \n",
    "    # ESTATÍSTICAS ACUMULADAS\n",
    "    print(\"=\" * 70)\n",
    "    print(\"📊 ESTATÍSTICAS DE USO\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"\\nTotal de Tokens: {cb.total_tokens}\")\n",
    "    print(f\"  ├─ Tokens de Prompt (input): {cb.prompt_tokens}\")\n",
    "    print(f\"  └─ Tokens de Completion (output): {cb.completion_tokens}\")\n",
    "    print(f\"\\nNúmero de Chamadas: {cb.successful_requests}\")\n",
    "    print(f\"Custo Total: ${cb.total_cost:.6f} USD\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01850919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALERTA DE LIMITE DIÁRIO\n",
    "LIMITE_DIARIO = 5.00  # $5 por dia\n",
    "custo_hoje = 0\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    resposta = llm.invoke(pergunta)\n",
    "    custo_hoje += cb.total_cost\n",
    "    \n",
    "    if custo_hoje > LIMITE_DIARIO:\n",
    "        raise Exception(f\"⚠️ LIMITE DIÁRIO EXCEDIDO: ${custo_hoje:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fca106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta: Quem criou o LangChain e quando?\n",
      "Resposta: LangChain foi criado por Harrison Chase em outubro de 2022.\n",
      "\n",
      "Pergunta: Qual a capital da França?\n",
      "Resposta: Não tenho essa informação no contexto fornecido.\n"
     ]
    }
   ],
   "source": [
    "# RAG - Recuperação + Geração\n",
    "# Conhecimento Atualizado\n",
    "# Fontes Verificáveis\n",
    "# Reduz Alucinações\n",
    "# Conhecimento Privado\n",
    "# QUANDO USAR:\n",
    "# Q&A sobre documentos (manuais, PDFs, wikis)\n",
    "# Chatbots com conhecimento específico da empresa\n",
    "# Precisa citar fontes\n",
    "# Conhecimento muda frequentemente\n",
    "# Dados proprietários/privados\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    http_client=GLOBAL_HTTP_CLIENT\n",
    ")\n",
    "\n",
    "# Simular base de conhecimento\n",
    "# Em produção, usar bases reais (DB, arquivos, etc)\n",
    "documentos = [\n",
    "    Document(page_content=\"LangChain foi criado por Harrison Chase em outubro de 2022.\"),\n",
    "    Document(page_content=\"LCEL significa LangChain Expression Language, introduzido em 2023.\"),\n",
    "    Document(page_content=\"Chains permitem compor múltiplas chamadas a LLMs e outras ferramentas.\")\n",
    "]\n",
    "\n",
    "# Template RAG: Instruir o modelo a usar APENAS o contexto fornecido\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Contexto:\\n{contexto}\\n\\n\"\n",
    "    \"Pergunta: {pergunta}\\n\\n\"\n",
    "    \"Instruções: Responda baseando-se EXCLUSIVAMENTE no contexto fornecido acima. \"\n",
    "    \"Se a informação não estiver no contexto, diga 'Não tenho essa informação no contexto fornecido.'\"\n",
    ")\n",
    "\n",
    "chain = template | llm\n",
    "\n",
    "# RECUPERAR documentos relevantes (aqui simplificado - pegamos todos)\n",
    "contexto_recuperado = \"\\n\".join([doc.page_content for doc in documentos])\n",
    "\n",
    "# Fazer pergunta\n",
    "resposta = chain.invoke({\n",
    "    \"contexto\": contexto_recuperado,\n",
    "    \"pergunta\": \"Quem criou o LangChain e quando?\"\n",
    "})\n",
    "\n",
    "print(\"Pergunta: Quem criou o LangChain e quando?\")\n",
    "print(f\"Resposta: {resposta.content}\\n\")\n",
    "\n",
    "# Testar outra pergunta fora do contexto\n",
    "resposta2 = chain.invoke({\n",
    "    \"contexto\": contexto_recuperado,\n",
    "    \"pergunta\": \"Qual a capital da França?\"\n",
    "})\n",
    "\n",
    "print(\"Pergunta: Qual a capital da França?\")\n",
    "print(f\"Resposta: {resposta2.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7c3ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_pdfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Carregar documentos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m docs = \u001b[43mload_pdfs\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mpasta_manuais/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Dividir em chunks (pedaços menores)\u001b[39;00m\n\u001b[32m      5\u001b[39m chunks = split_documents(docs, chunk_size=\u001b[32m500\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_pdfs' is not defined"
     ]
    }
   ],
   "source": [
    "#INGESTÃO DE CONTEÚDOS\n",
    "\n",
    "# Carregar documentos\n",
    "docs = load_pdfs(\"pasta_manuais/\")\n",
    "\n",
    "# Dividir em chunks (pedaços menores)\n",
    "chunks = split_documents(docs, chunk_size=500)\n",
    "\n",
    "# Criar embeddings (representações vetoriais)\n",
    "embeddings = create_embeddings(chunks)\n",
    "\n",
    "# Salvar em vector store (banco de dados vetorial)\n",
    "vectorstore.add(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf53456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query Time (Quando usuário pergunta):\n",
    "\n",
    "# Usuário faz pergunta\n",
    "pergunta = \"Como resetar a senha?\"\n",
    "\n",
    "# Buscar chunks similares (similarity search)\n",
    "docs_relevantes = vectorstore.similarity_search(pergunta, k=3)\n",
    "\n",
    "# Montar contexto com docs encontrados\n",
    "contexto = \"\\n\".join([doc.page_content for doc in docs_relevantes])\n",
    "\n",
    "# LLM responde baseado nos docs\n",
    "resposta = chain.invoke({\"contexto\": contexto, \"pergunta\": pergunta})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4e592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  RAG com Metadados\n",
    "\n",
    "documentos = [\n",
    "    Document(\n",
    "        page_content=\"Python foi criado por Guido van Rossum em 1991.\",\n",
    "        metadata={\"fonte\": \"historia_python.pdf\", \"pagina\": 1}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Python é uma linguagem interpretada e de alto nível.\",\n",
    "        metadata={\"fonte\": \"intro_python.pdf\", \"pagina\": 3}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Incluir fonte na resposta\n",
    "template = ChatPromptTemplate.from_template(\n",
    "    \"Contexto com fontes:\\n{contexto}\\n\\n\"\n",
    "    \"Pergunta: {pergunta}\\n\\n\"\n",
    "    \"Responda e cite a fonte da informação.\"\n",
    ")\n",
    "\n",
    "contexto_com_fontes = \"\\n\".join([\n",
    "    f\"[Fonte: {doc.metadata['fonte']}] {doc.page_content}\"\n",
    "    for doc in documentos\n",
    "])\n",
    "\n",
    "resposta = chain.invoke({\n",
    "    \"contexto\": contexto_com_fontes,\n",
    "    \"pergunta\": \"Quem criou Python?\"\n",
    "})\n",
    "\n",
    "print(resposta.content)\n",
    "# Resposta: \"Python foi criado por Guido van Rossum em 1991 (Fonte: historia_python.pdf)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
